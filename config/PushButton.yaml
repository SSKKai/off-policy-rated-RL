experiment:
  env_type: rlbench
  episode_len: 150
  max_episodes: 3300
  wandb_log: true


sac:
  policy: Gaussian #Policy Type: Gaussian | Deterministic (default: Gaussian)
  eval: true #Evaluates a policy a policy every 10 episode (default: True)
  eval_per_episode: 100 #evaluate policy per episode
  eval_episodes: 3 #number of evaluate episodes
  gamma: 0.99
  tau: 0.005 #target smoothing coefficient(τ) (default: 0.005)
  lr: 0.0003
  alpha: 0.2 #Temperature parameter α determines the relative importance of the entropy term against the reward (default: 0.2)
  automatic_entropy_tuning: true #Automaically adjust α (default: False)
  seed: 123456 #random seed (default: 123456)
  batch_size: 256
  hidden_size: 256
  updates_per_step: 1 #model updates per simulator step (default: 1)
  start_steps: 10000
  target_update_interval: 1 #Value target update per no. of updates per step (default: 1)
  replay_size: 1000000  #size of replay buffer (default: 10000000)
  cuda: true

reward:
  sample_method: random sample # random sample / distance sample
  padding_mask_method: zeros pad normal mask # zeros pad normal/shortest mask, last n/edge pad no mask
  label_type: adaptive #onehot / smoothing / adaptive
  prio_alpha: 3 # alpha for prioritized sampling
  best_trajs_num: 0 # a number
  rank_by_true_reward: true
  state_only: false #the reward net is r(s,a) or r(s)
  hidden_dim: 256 #hidden dim for reward network
  learn_reward_frequency: 100 #learn reward per N episodes
  num_to_rank: 10 #num to rank per reward update
  traj_capacity: 200 #trajectory capacity of reward buffer
  lr: 0.001

env:
  task: PushButton
  static_env: false
  headless_env: false
  obs_type: LowDimension
  state_dim: 11