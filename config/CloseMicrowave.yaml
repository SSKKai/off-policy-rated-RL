experiment:
  env_type: rlbench
  episode_len: 250
  max_episodes: 6000
  wandb_log: true


sac_hyperparams:
  policy: Gaussian #Policy Type: Gaussian | Deterministic (default: Gaussian)
  eval: true #Evaluates a policy a policy every 10 episode (default: True)
  eval_per_episode: 100 #evaluate policy per episode
  eval_episodes: 3 #number of evaluate episodes
  gamma: 0.99
  tau: 0.005 #target smoothing coefficient(τ) (default: 0.005)
  lr: 0.0003
  alpha: 0.2 #Temperature parameter α determines the relative importance of the entropy term against the reward (default: 0.2)
  automatic_entropy_tuning: true #Automaically adjust α (default: False)
  seed: 123456 #random seed (default: 123456)
  batch_size: 256
  hidden_size: 256
  updates_per_step: 1 #model updates per simulator step (default: 1)
  start_steps: 10000
  target_update_interval: 1 #Value target update per no. of updates per step (default: 1)
  replay_size: 1000000  #size of replay buffer (default: 10000000)
  cuda: true

reward_hyperparams:
  sample_method: random sample #Sample method for sampling a batch of trajectories to get ranked
  padding_mask_method: zeros pad normal mask # zeros pad shortest mask, edge pad no mask, last n pad no mask
  rank_by_true_reward: true
  state_only: False #the reward net is r(s,a) or r(s)
  hidden_dim: 256 #hidden dim for reward network
  learn_reward_frequency: 100 #learn reward per N episodes
  num_to_rank: 10 #num to rank per reward update
  traj_capacity: 200 #trajectory capacity of reward buffer
  lr: 0.001

rlb_env_config:
  task: CloseMicrowave
  static_env: False
  headless_env: False
  obs_type: LowDimension
  state_dim: 19